\documentclass[11pt]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{times}
\usepackage{amsmath,amsthm,amssymb,setspace,enumitem,epsfig,titlesec,verbatim,array,eurosym,multirow}
\usepackage[sort&compress,numbers]{natbib}
\usepackage[footnotesize,bf]{caption}
\usepackage[margin=2.5cm, includefoot, footskip=30pt]{geometry}
\usepackage{standalone}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{blkarray}
\usepackage[ruled,vlined]{algorithm2e}
\smallskip % Erlaubt kleine Abstaende zwischen Paragraphen, falls es dem Seitenlayout hilft
\renewcommand{\baselinestretch}{1.15}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{minitoc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{standalone}

\newcommand{\nikoleta}[1]{\textcolor{orange}{\textbf{NG}: #1}}
\newcommand{\christian}[1]{\textcolor{blue}{\textbf{CH}: #1}}

\newcommand{\figref}[1]{{\textbf{Fig.~\ref{#1}}}}

\def\wsls{\texttt{WSLS}}
\def\tft{\texttt{TFT}}
\def\tftt{\texttt{TF2T}}
\def\ttft{\texttt{2TFT}}
\def\gtft{\texttt{GTFT}}
\def\allc{\texttt{ALLC}}
\def\alld{\texttt{ALLD}}
\def\alt{\texttt{Alternator}}
\def\SI{\textbf{Supporting Information}}


\titleformat{\section}{\sffamily \fontsize{12}{15}\bfseries}{\thesection}{0.4em}{}
\titleformat{\subsection}{\sffamily\fontsize{11}{15}\bfseries}{\thesubsection}{0.4em}{}




\title{\bfseries \sffamily \Large Conditional coperation with longer memory}

%% Perhaps: Better title (Conditional cooperation with longer memory) %% 

\author{Nikoleta E. Glynatsi$^{1,*}$, Martin Nowak$^2$, Christian Hilbe$^1$\\[0.3cm]
$^{\rm 1}$Max Planck Research Group on the Dynamics of Social Behavior,\\ Max Planck Institute for Evolutionary Biology, Pl\"{o}n, Germany \\
$^{\rm 2}$Department of Mathematics,
Department of Organismic and Evolutionary Biology,\\ Harvard University, Cambridge, USA\\
$^*$To whom correspondence should be addressed. E-mail: glynatsi@evolbio.mpg.de
}
\date{}

\begin{document}

\maketitle

~\\[0.5cm]
\noindent
{\bf 
Repeated games enable evolution of cooperation if players use conditional
strategies that depend on previous interactions. A well known strategy set is
given by reactive strategies, which respond to the co-player's previous move.
Here we extend reactive strategies to longer memories. A reactive-$n$ strategy
takes into account the sequence of the $n$ last moves of the co-player. A
reactive-$n$ counting strategy takes into account how often the co-player has
cooperated during the last $n$ round. We characterize all partner strategies among
reactive-2 and reactive-3 strategies as well as among reactive-$n$ counting
strategies. Partner strategies are those that ensure mutual cooperation without
exploitation. We perform evolutionary simulations and find that longer memory
increases the average cooperation rate for reactive-$n$ strategies but not for
reactive counting strategies.
}\\[1cm]


\noindent
{\it Keywords:} Evolutionary game theory, direct reciprocity, evolution of cooperation, prisoner's dilemma



\clearpage
\newpage


%%%%%%%%%%%%%
%% SIGNIFICANCE  %%
%%%%%%%%%%%%%

\noindent
{\bf Significance statement.} Humans tend to cooperate conditionally. 
We are often influenced by how cooperative others are, and we adapt our behavior accordingly. 
To describe reciprocal cooperation, theoretical models often presume that individuals only react to their last interaction. 
Instead, here we allow individuals to react to an opponent's $n$ previous actions, for arbitrary $n$.
We derive an algorithm to identify all partner strategies -- strategies that sustain full cooperation in a Nash equilibrium.
We give explicit conditions for $n\!=\!2$ and $n\!=\!3$. 
When individuals only count how often their opponent cooperated, independent of the timing of cooperation, we characterize partner strategies for all $n$.
%Additional simulations confirm that partner strategies are crucial for the evolution of cooperation.\\
These results clarify which strategies sustain cooperation under more realistic assumptions on people's cognitive abilities. 



%%%%%%%%%%%%%%
%%  INTRODUCTION  %%
%%%%%%%%%%%%%%

\section*{Introduction}

To a considerable extent, human cooperative behavior is governed by direct reciprocity~\cite{melis:ptrs:2010,rand:TCS:2013}. 
This mechanism for cooperation can explain why people return favors~\citep{neilson:JEBO:1999}, why they show more effort in group tasks when others do~\citep{fischbacher:AER:2010}, or why they stop cooperating when they feel exploited~\citep{hilbe:ncomms:2014,Xu:NComms:2016}. 
The main theoretical framework to describe reciprocity is the repeated prisoner's dilemma~\cite{axelrod:AAAS:1981, nowak:Science:2006, sigmund2010,Garcia:FRAI:2018,hilbe:Nature:2018}. 
This game considers two individuals, referred to as players, who repeatedly decide whether to cooperate or to defect with one another~(\figref{fig:conceptual_figure_model}\textbf{A}). 
Both players prefer mutual cooperation to mutual defection. 
Yet given the co-player's action, each player has an incentive to defect. 
One common implementation of the prisoner's dilemma is the donation game. 
Here, cooperation simply means to pay a cost $c\!>\!0$ for the co-player to get a benefit $b\!>\!c$. 
Despite the simplicity of these games, they can give rise to remarkable dynamical patterns that have been explored in numerous studies~\citep{frean:PRSB:1994,killingback:PRSB:1999,hauert:JTB:2002b,kurokawa:PRSB:2009,pinheiro:PLoSCB:2014,garcia:jet:2016,mcavoy:PRSA:2019,Kraines:TaD:1989,nowak:Nature:1993,imhof:PNAS:2005,grujic:jtb:2012,van-segbroeck:prl:2012,press:PNAS:2012,stewart:pnas:2013,Toupo:IJBC:2014,stewart:pnas:2014, akin:EGADS:2016,glynatsi:scientific:2020,chen:PNASnexus:2023}.
Some of this literature describes how the evolution of cooperation depends on the game parameters, such as the benefit of cooperation, or the frequency with which errors occur~\citep{boyd:JTB:1989,Hao:PRE:2015,Zhang:GEB:2018,Mamiya:PRE:2020}. 
Others describe the effect of different learning dynamics~\citep{stewart:games:2015,Mcavoy:PNAS:2022}, of population structure~\citep{brauchli:JTB:1999,szabo:PRE:2000b,allen:AmNat:2013,szolnoki:scirep:2014}, or of the strategies that players are permitted to use~\citep{baek:scientific:2016}.  

Strategies of the repeated prisoner's dilemma can vary in their complexity.
While some are straightforward to implement, like always defect, others are more sophisticated~\cite{harper:PLOSONE:2017, knight:PLOSONE:2018}.
To quantify a strategy's complexity, it is common to resort to the number of past rounds that the player needs to remember. 
Unconditional strategies like always defect or always cooperate are said to be \mbox{memory-0}. 
Strategies that only depend on the previous round, such as Tit-for-Tat~~\cite{axelrod:AAAS:1981,Duersch:IJGT:2013} or Win-Stay Lose-Shift~\cite{Kraines:TaD:1989,nowak:Nature:1993}, are \mbox{memory-1}~~(\figref{fig:conceptual_figure_model}\textbf{B}). 
Similarly, one can distinguish strategies that require more than one round of memory, or strategies that cannot be implemented with finite memory~\cite{Garcia:FRAI:2018}. 

Traditionally, most theoretical research on the evolution of reciprocity focuses on memory-1 strategies~\citep{nowak:Nature:1993,imhof:PNAS:2005,grujic:jtb:2012,van-segbroeck:prl:2012,press:PNAS:2012,stewart:pnas:2013,Toupo:IJBC:2014,stewart:pnas:2014, akin:EGADS:2016,glynatsi:scientific:2020,chen:PNASnexus:2023}. 
Although one-round memory can explain some of the empirical regularities in human behavior~\cite{engle:ET:2006, dal:AER:2011, camera:GEB:2012, bruttel:TD:2012,Montero-Porras:SciRep:2022},  people often take into account more than the last round~\cite{romero:EER:2018}.
Longer memory seems particularly relevant for noisy games, where people occasionally defect because of unintended errors~\cite{fudenberg:AER:2012}. 
However, a formal analysis of strategies with more than one-round memory has been difficult for two reasons. 
First, as the memory length $n$ increases, strategies become harder to interpret. 
For example, because two consecutive rounds of the prisoner's dilemma allow for 16 possible outcomes, memory-2 strategies need to specify 16 conditional cooperation probabilities~\citep{hauert:PRSB:1997}. 
Although some of the resulting strategies have an intuitive interpretation, such as Tit-for-Two-Tat~\citep{axelrod:AAAS:1981}, many others are difficult to make sense of. 
Second, the number of strategies, and the time it takes to compute their payoffs, increases dramatically in $n$. 
For example, for $n\!=\!1$, there are $2^4\!=\!16$ deterministic strategies (strategies that do not randomize between different actions). 
Computing the payoffs of players with one-round memory requires the inversion of a $4\!\times\!4$ matrix~\cite{sigmund2010}. 
After increasing the memory length to $n\!=\!2$, there are $2^{16}\!=\!64,536$ deterministic strategies, and payoffs now require the inverse of a $16\!\times\!16$ matrix. 
Probably for these reasons, previous studies were either restricted to simulations for small $n$~\citep{hauert:PRSB:1997,stewart:scientific:2016,Murase:PLoSCompBio:2023a}, or they analyzed the properties of a few particular higher-memory strategies~\citep{hilbe:PNAS:2017,ueda:RSOP:2021,li:NatureCompSci:2022}. 

To make progress, we focus on an easy-to-interpret subset of memory-$n$ strategies, the {\it reactive}-$n$ strategies. 
Capturing the basic premise of conditional cooperation, they only depend on the {\it co-player's} actions during the last $n$ rounds~(\figref{fig:conceptual_figure_model}\textbf{C,E}). 
While it has been difficult to explicitly characterize all Nash equilibria among the memory-$n$ strategies, we show that such a characterization is possible for reactive-$n$ strategies. 
Our results rely on a central insight, motivated by previous work by Press~\&~Dyson~\citep{press:PNAS:2012}: 
if one player adopts a reactive-$n$ strategy, the other player can always find a best response among the deterministic {\it self-reactive}-$n$ strategies. 
Self-reactive-$n$ strategies are remarkably simple. 
They only depend on the player's own previous $n$ moves (\figref{fig:conceptual_figure_model}\textbf{D,F}).
Based on this insight, we look for all reactive-$n$ strategies that sustain full cooperation in a Nash equilibrium (the so-called {\it partner strategies}). 
We provide a full characterization for $n\!=\!2$ and $n\!=\!3$.
Even stronger results are feasible when we restrict attention to so-called {\it counting strategies}.  
Such strategies only react to how often the co-player has cooperated in the last $n$ rounds (irrespective of the exact timing of cooperation). 
For the donation game, we characterize the partners among the counting strategies for arbitrary~$n$. 
The resulting conditions are straightforward to interpret:
For every defection of the co-player in memory, the focal player's cooperation rate needs to drop by $c/(nb)$.
To further assess the relevance of partner strategies for the evolution of cooperation, we conduct extensive simulations for $n\!\in\!\{1,2,3\}$. 
Our findings indicate that the evolutionary process strongly favors partner strategies, and that these strategies are crucial for cooperation. 

Overall, our results provide important insights into the logic of conditional cooperation when players have more than one-round memory. 
We show that partner strategies exist for all repeated prisoner's dilemmas and for all memory lengths. 
To be stable, however, these strategies need to be sufficiently responsive to the co-player's previous actions. 


%%%%%%%%%%
%% RESULTS  %%
%%%%%%%%%%


\section*{Results}

%%%%%%
% Model %
%%%%%%

\textbf{Model and notation.}
We consider a repeated game between two players, player~1 and player~2.
Each round, players can choose to cooperate ($C$) or to defect ($D$). 
If both players cooperate, they receive the payoff $R$ (reward), which exceeds the (punishment) payoff~$P$ for mutual defection. 
If only one player cooperates, the defector receives the temptation~$T$, whereas the cooperator ends up with the sucker's payoff~$S$. 
The players' aim is to maximize their average payoff per round, across infinitely many rounds.
We assume payoffs satisfy the typical relationships of a prisoner's dilemma, $T \!>\! R \!>\! P \!>\! S$ and $2 R \!>\! T \!+\! S$. 
Therefore, in each round, mutual cooperation is the best outcome for the pair, but players have some incentive to defect. 
To make results easier to interpret, it is sometimes instructive to look at a particular variant of the prisoner's dilemma, the donation game. 
Here, cooperation means to pay a cost $c\!>\!0$ for the co-player to get a benefit $b\!>\!c$.
The resulting payoffs are \(R \!\!=\! b\! -\! c, S \!=\! -c, T \!=\! b, P\!
=\! 0\). 
We use this donation game in the following to illustrate our results. 
However, most of our findings are straightforward to extend to the general prisoner's dilemma (or to other repeated $2\!\times\!2$ games, see \SI). 

We consider players who use strategies with finite memory. 
To describe such strategies formally, it is useful to introduce some notation. 
An {\it $n$-history for player $i\! \in\! \{1, 2\}$} is a tuple $\mathbf{h}^i\!=\!(a^i_{-n},\ldots,a^i_{-1})\!\in\!\{C,D\}^n$. 
Each entry $a^i_{-k}$ corresponds to player $i$'s action $k$ rounds ago. 
We use $H^i$ for the set of all such $n$-histories. 
This set contains $|H^i|\!=\!2^{n}$ elements. 
Based on this notation, we can define a {\it reactive}-$n$ {\it strategy} for player 1 as a vector $\mathbf{p}\!=\!(p_\mathbf{h})_{\mathbf{h}\in H^2} \!\in\! [0, 1]^{2^n}$. 
The entries $p_\mathbf{h}$ correspond to player 1's cooperation probability in any given round, contingent on player 2's actions during the last $n$ rounds. 
The strategy is called pure or deterministic if any entry is either zero or one. 
We note that the above definition leaves player 1's moves during the initial $n$ rounds unspecified. 
However, in infinitely repeated games without discounting, these initial moves tend to be inconsequential. 
Hence, we will neglect them in the following.

For \(n\!=\!1\), the above definition recovers the classical format of reactive-1 strategies~\cite{sigmund2010}, \(\mathbf{p}\!=\!(p_C, p_D)\). 
This set contains, for example, the strategies of unconditional defection, \alld~$=\!(0,0)$, and Tit-for-Tat, \tft~$=\!(1,0)$. 
The next complexity class is the set of reactive-2 strategies, $\mathbf{p}\!=\!(p_{CC},p_{CD},p_{DC},p_{DD})$.
In addition to \alld{} and \tft{}, this set contains, for instance, the strategies Tit-for-Two-Tat, \tftt~$=\!(1,1,1,0)$ and Two-Tit-for-Tat,~\ttft$=\!(1,0,0,0)$. 
Similar examples exist for $n\!>\!2$. 
When both players adopt reactive-$n$ strategies (or more generally, memory-$n$ strategies), it is straightforward to compute their expected payoffs, by representing the game as a Markov chain. 
The respective procedure is described in the \SI{}.  

Herein, we are particularly interested in those reactive-$n$ strategies that sustain full cooperation. 
Such strategies ought to have two properties. 
First, such a strategy ought to be {\it nice}, meaning that it should never be the first to defect~\citep{axelrod:AAAS:1981}.
This property ensures that two players with that strategy would fully cooperate. 
In particular, if $\mathbf{h}_C$ is a co-player's $n$-history that consists of $n$ bits of cooperation, a nice strategy needs to respond by cooperating with certainty, $p_{\mathbf{h}_C}\!=\!1$.  
Second, the strategy ought to form a {\it Nash equilibrium}, such that no co-player has an incentive to deviate. 
Strategies that have both properties are called {\it partner strategies}~\citep{Hilbe:GEB:2015} or {\it partners}.
The partners among the reactive-1 strategies are well known. 
For the donation game, partners are those strategies with $p_C\!=\!1$ and $p_D\!\le\!1\!-\!c/b$~\citep{akin:EGADS:2016}. 
However, a general theory of partners for $n\!\ge\!2$ is lacking. 
This is what we aim to derive in the following. 
In the main text, we provide the main intuition for the respective results; all proofs are in the \SI.\\
 
 
 
 %%%%%%%%
 %  Algorithm  %
 %%%%%%%%
 
\noindent
\textbf{An algorithm to identify partners among the reactive-$n$ strategies.} 
It is comparably easy to verify whether a reactive-$n$ strategy $\mathbf{p}$ is nice. 
Demonstrating that the strategy is also a Nash equilibrium, however, is far less trivial. 
In principle, this requires uncountably many payoff comparisons. 
We would have to show that if player 2's strategy is fixed to $\mathbf{p}$, no other strategy $\sigma$ for player~1 can result in a higher payoff. That is, player 1's payoff needs to satisfy $\pi^1(\sigma,\mathbf{p})\!\le\!\pi^1(\mathbf{p},\mathbf{p})$ for all $\sigma$. 
Fortunately, this task can be simplified considerably. 
Already Press~\& Dyson~\cite{press:PNAS:2012} showed that it is sufficient to check only those $\sigma$ with at most $n$ rounds of memory. 
Based on two insights, we can even further restrict the search space of strategies $\sigma$ that need to be tested.

First, suppose player~1 uses some arbitrary strategy $\sigma$ against player~2 with reactive-$n$ strategy \mbox{$\mathbf{p}\!=\!(p_\mathbf{h})_{\mathbf{h}\in H^1}$}. 
Then we prove that instead of $\sigma$, player~1 may switch to a {\it self-reactive}-$n$ strategy $\mathbf{\tilde{p}}$ without changing either player's payoffs. 
When adopting a self-reactive strategy, player~1 only takes into account her own actions during the last $n$ rounds, 
$\mathbf{\tilde{p}} \!=\! (\tilde{p}_\mathbf{h})_{\mathbf{h} \in H^1}$.
In particular, if $\sigma$ is a best response to $\mathbf{p}$, then the associated self-reactive strategy $\mathbf{\tilde p}$ is also a best response. 
This result follows a similar intuition as the corresponding result of Press \& Dyson~\cite{press:PNAS:2012}: 
if there is a part of the joint history that player~2 does not take into account, player~1 gains nothing by considering that part of the history. 
In our case, because player~2 only considers the last $n$ actions of player~1, it is sufficient for player~1 to do the same.
\figref{fig:conceptual_figure_results}\textbf{A,B} provides an illustration.  
There, we depict a game in which player~\textcolor{red}{1} % It would be nice to change the figure accordingly. 
adopts a memory-1 strategy against a reactive-1 opponent. 
Due to the above result, we can find an equivalent self-reactive-1 strategy for player~1. 
While that self-reactive strategy is simpler, on average it induces the same game dynamics. 
Hence, it results in identical payoffs. 

The above result guarantees that for any reactive-$n$ strategy, there is always a best response among the self-reactive-$n$ strategies. 
In a second step, we prove that such a best response can always be found among the {\it deterministic} self-reactive-$n$ strategies. 
This reduces the search space for potential best responses further, from an uncountable set to a finite set of size $2^{2^n}$. 
For $n\!=\!2$, this leaves us with 16 self-reactive strategies to test. 
For $n\!=\!3$, we end up with (at most) 256 strategies.\\


%%%%%%%%%%%%
% Partners for n=2,3  %
%%%%%%%%%%%%

\noindent
\textbf{Partners among the reactive-2 and the reactive-3 strategies.}
To illustrate the above algorithm, we first characterize the partners among the reactive-$2$ strategies. 
To this end, we note that it is straightforward to compute the payoff of a specific self-reactive-2 strategy against a general reactive-2 strategy $\mathbf{p}$ (see \SI{} for details). 
By computing the payoffs of all 16 pure self-deterministic strategies $\mathbf{\tilde p}$, and by requiring $\pi^1(\mathbf{\tilde p},\mathbf{p}) \!\le\! \pi^1(\mathbf{p},\mathbf{p})$ for all of them, we conclude that $\mathbf{p}$ is a partner if and only if
\begin{equation}\label{eq:two_bit_conditions}
  p_{CC} = 1, \qquad  \frac{p_{CD} + p_{DC}}{2} \le 1 - \frac{1}{2} \!\cdot\! \frac{c}{b}, \qquad  p_{DD} \leq 1\!-\! \frac{c}{b}.
\end{equation}

Hence, for a strategy to be a Nash equilibrium, it must ensure that the strategy
ALLD doesn't yield a higher payoff (achieved by $p_{DD} \leq 1 - \frac{c}{b}$),
and the average cooperation rate after a single defection by the co-player in
the last two rounds must be less than half the cost-benefit ratio ($c/b$). These
conditions define partner strategies as a three-dimensional polyhedron within
the space of all nice reactive-2 strategies
(Fig.~\ref{fig:conceptual_figure_results}C).

A reactive-3 strategy is defined by the vector $\mathbf{p} = (p_{CCC}, p_{CCD},
p_{CDC}, p_{CDD}, p_{DCC}, p_{DCD}, p_{DDC}, p_{DDD})$, and it is a partner strategy,
if and only if the strategy entries satisfy the conditions,

\begin{align}\label{eq:three_bit_conditions}
  \begin{split}
  p_{CCC} & = 1 \\
  \frac{p_{CDC} + p_{DCD}}{2} & \leq 1 - \frac{1}{2} \cdot \frac{c}{b} \\
  \frac{p_{CCD} + p_{CDC} + p_{DCC}}{3} & \leq 1 - \frac{1}{3} \cdot \frac{c}{b} \\
  \frac{p_{CDD} + p_{DCD} + p_{DDC}}{3} & \leq 1 - \frac{2}{3} \cdot \frac{c}{b} \\
  \frac{p_{CCD} + p_{CDD} + p_{DCC} + p_{DDC}}{4}  & \leq 1 - \frac{1}{2} \cdot \frac{c}{b}  \\
  p_{DDD} & \leq 1\!-\! \frac{c}{b}
  \end{split}
\end{align}

Inherently, these conditions still exhibit some symmetry with the case of
reactive-2. Namely, for the strategy to be Nash, ALLD should not achieve a
higher payoff. Additionally, the average cooperation following a single
defection must be lower than 2/3 of the cost-benefit ratio, and the average
cooperation following two defections must be smaller than 1/3 of the
cost-benefit ratio. However, there are two further conditions that appear not to
align with this intuition. We hypothesize that as the memory space we allow
increases, the number of conditions will also increase, and some of these
conditions will deviate from the symmetry. Note that the two additional conditions ensure
that strategies playing the sequence of actions $CCDD$ and $CD$ cannot exploit the
strategy.

The proofs for the above results can be found in the Supplementary Information.
In addition to demonstrating the results using the methodology we have described
in the paper, we can also verify them using an independent proof. This
independent proof builds upon the framework developed by
Akin~\cite{akin:EGADS:2016}.

In the Supplementary Information we derive the conditions for partner strategies
for the general Prisoner's Dilemma for reactive-2 and reactive-3 strategies. In
Fig.~\ref{fig:conceptual_figure_results}D, we plot the space of partner
strategies for $n=2$ and for $R =3, S=0, T=5, P=1$.

\textbf{Partner Strategies Amongst Reactive Counting Strategies}
A special case of reactive strategies is reactive counting strategies. These are
strategies that respond to the co-player's actions, but they do not distinguish
between when cooperations occurred in the last $n$ turns; they solely consider
the count of cooperations. A reactive-$n$ counting strategy is represented by a
vector $\mathbf{r}=(r_i)_{i \in \{n, n -1, \dots, 0\}}$, where the entry \(r_i\)
indicates the probability of cooperating given that the co-player cooperated
\(i\) times in the last \(n\) turns. Note that a reactive-1 strategy
$\mathbf{p}=(p_{C}, p_{D})$ and a counting strategy $\mathbf{r}=(r_1, r_0)$ are
equivalent because both strategies describe the probability of cooperating after
a single or no cooperation by the co-player through their respective entries. 

A reactive-2 counting strategy is denoted by the vector $\mathbf{r}=(r_2,
r_1, r_0)$, and we can characterise partner strategies among the reactive-2
counting strategies by simply setting $r_2 = 1$, and $p_{CD} = p_{DC} = r_1$ and
$p_{DD} = r_0$ in conditions~\eqref{eq:two_bit_conditions} which gives us the
following conditions,

\begin{equation}\label{eq:counting_two_bit_conditions}
  r_2 = 1, \quad \displaystyle r_1 < 1-\frac{1}{2} \cdot \frac{c}{b} ~~and~~ r_0 < 1\!-\! \frac{c}{b}.
\end{equation}

Similarly, a reactive-3 counting strategy is denoted by the vector
$\mathbf{r}=(r_3, r_2, r_1, r_0)$, and we characterise partner strategies among
reactive-3 counting strategies by setting $r_3 = 1$, and $p_{CCD} = p_{CDC} =
p_{DCC} = r_2, p_{DCD} = p_{DDC} = p_{CDD} = r_1$ and $p_{DDD} = r_0$ in
conditions~\eqref{eq:three_bit_conditions}. This gives us the following
conditions,

\begin{equation}\label{eq:counting_three_bit_conditions}
  r_3 = 1, \quad \displaystyle r_2 < 1- \frac{1}{3} \cdot \frac{c}{b}, \quad r_1 < 1- \frac{2}{3} \cdot \frac{c}{b} ~~and~~ r_0 < 1\!-\! \frac{c}{b}.
\end{equation}

Counting strategies are a subset of reactive strategies, and as such, they exist
within the space of reactive partner strategies. For example, in the case of
$n=2$, the counting partner strategies form a plane within the three-dimensional
polyhedron of reactive-2 partners (Fig.~\ref{fig:conceptual_figure_results}B).
Counting partner strategies appear to align with the intuition that the
generosity (the probability of cooperating after a defection, thus being
generous with your co-player) exhibited by a strategy after a $k$ number of
defections in the last $n$ rounds must be less than $1 - k/n$ of the
cost-benefit ratio. As the total number of defections increases, the strategy's
generosity decreases. And, precisely, this is the result we prove (see
Supplementary Information). In the case of reactive-counting strategies, we
characterize partner strategies for all memory lengths. A reactive-counting
strategy is a partner if and only if,

\begin{equation}
  r_n = 1 ~~and~~ r_{n - k} < 1 - \frac{k}{n} \cdot \frac{c}{b}, \text{ for } k \in \{1, 2, \dots, n\}.
\end{equation}


\textbf{Evolutionary Dynamics.}
Based on our previous equilibrium analysis, we know the conditions that a
reactive strategy must satisfy to be considered a partner strategy. The next
step is to determine whether these strategies are likely to evolve through an
evolutionary process. Additionally, what remains unclear is the impact of
increased memory, as well as the consequences of limiting strategies to counting
alone. Here, we will empirically explore these questions by simulating an
imitation process, using the framework described by Imhof and
Nowak~\cite{imhof:royal:2010}. The setup of the framework is outlined in
Materials and Methods~\ref{section:materials_and_methods}.

First, we explore which strategies evolve from the evolutionary dynamics for a
fixed set of parameters. We ran 10 independent simulations for each set of
strategies and recorded the resident strategy at each elementary time step. Once
a strategy has become a resident we also record the number of time steps it
remained a resident. Thus, the number of mutants that have unsuccessfully tried
to invade the resident population. In Fig.~\ref{fig:evolutionary_results}A and B
we represent those strategies that repelled the highest number of mutant in each
run. We call these strategies the {\it most abundant}.
Fig.~\ref{fig:evolutionary_results}A shows the most abundant strategies for
reactive strategies and Fig.~\ref{fig:evolutionary_results}B shows the most
abundant strategies for counting strategies. In both cases the most abundant
strategies resemble partner strategies. In the case of counting strategies, we
can see the decreasing levels of forgiveness as the number of cooperations
decreases.

Next, we compare the evolution of partner strategies and the changing
cooperation rates for different memory sizes while varying the selection
strength. To this end, we ran simulations for different $b/c$ ratios.
As we examine the impact of memory size on the evolution of partner
strategies, several patterns emerge. Increasing memory size tends to result
in a higher abundance of partner strategies, regardless of the selection
strength. Notably, the highest abundance is observed for lower cost values. We
notice that the curves representing evolving cooperation rates align with the
prevalence of partner strategies. Thus, it is the presence of partner strategies
that facilitates the evolution of cooperation, and as memory selects partner
strategies more frequently, the cooperation rate also increases with memory.
In contrast, when examining
counting strategies, we notice that the abundance of partner strategies rise
with the strength of selection. However, there is no corresponding increase as
memory size expands. Thus, in the case of counting strategies there is no added
value in increasing memory size, from an evolutionary perspective.


\section*{Discussion}

Previous theoretical research has mainly focused on a single set of strategies
in repeated games, namely, memory-1 strategies. Although several results have
been proven for this class, generalizing to larger memory classes has proven to
be a challenging task. We venture into the realm of higher memory strategies by
concentrating on reactive strategies. Reactive strategies are a set that
observes only the previous turns of the co-player. They have been studied in the
past in theoretical work, with famous strategies such as Tit for Tat and
Generous Tit for Tat~\cite{nowak:Nature:1993}. Experimental research has even
suggested that these strategies are adopted by humans~\cite{engle:ET:2006,
bruttel:TD:2012}. However, prior work on reactive strategies has also been
limited to the case of memory one.

We focus on a set of Nash equilibria, which are the partner strategies. Partner
strategies not only ensure that their co-player has no reason to deviate but
also that as long as the co-player wants to, the payoff of mutual cooperation
can be achieved. Partner strategies are a set of strategies that
allow for evolution of cooperation~\cite{hilbe:Nature:2018},
which is also verified by our own work.

We begin by proving the result that if a player employs a reactive strategy,
then the co-player using a memory-$n$ strategy can switch to a self-reactive-$n$
strategy without altering the resulting payoffs. This result makes it easier for
us to characterize Nash strategies within the reactive set. We characterize
partner strategies for reactive-2 and reactive-3, both in the special case of
the donation game and the general Prisoner's Dilemma. Moreover, we also
demonstrate that reactive strategies such as Tit For Tat, Generous Tit For Tat,
and any delayed version of them are partner strategies (see Supplementary
Information).

We also focus on the set of counting strategies. In this case, we can easily
derive the condition for being a partner for $n=2$ and $n=3$. Furthermore,
counting strategies allow us to characterize all partner strategies regardless
of the memory size. The conditions for being partner in the counting set are
simple yet novel. The intuition of these conditions is that the generosity shown
by a partner strategy after a sequence of $k$ defections in the last $n$ rounds
must be less than $1 - k/n$ of the cost-benefit ratio. This condition ensures
that as the total number of defections increases, the strategy's generosity
decreases.

When testing the evolutionary properties of counting strategies, it is evident
from the simulation results that cooperation cannot emerge beyond the simple
case of reactive-1 strategies. Thus, we observe that within the reactive set,
the evolution of cooperation relies on the sequential memory of these strategies.
Overall, our study is among the first to characterize full spaces of partner
strategies in higher memory spaces. Although reactive strategies are a subset of
memory strategies, we have demonstrated that there are many results to explore
in this case.

\section*{Materials and Methods}\label{section:materials_and_methods}

In the following paragraphs, we describe the framework of our evolutionary
process. The framework considers a population of size \(N\) where initially all
members are of the same strategy. In our case the initial population consists of
unconditional defectors. In each elementary time step, one individual switches
to a new mutant strategy. The mutant strategy is generated by randomly drawing
cooperation probabilities from the unit interval \([0,1]^n\). If the mutant
strategy yields a payoff of \(\pi_{M, k}\), where \(k\) is the number of
mutants in the population, and if residents get a payoff of \(\pi_{R,
k}\), then the fixation probability \(\phi_{M}\) of the mutant strategy can be
calculated explicitly,

\begin{equation}\label{eq:fixation_probability}
  \phi_{M} = \frac{1}{\left(1 + \displaystyle \sum_{i=1}^{N - 1} \prod_{j=1}^{i} e^{(- \beta (\pi_{M, j} - \pi_{R, i}))} \right)}
\end{equation}

The parameter \(\beta \geq 0\) is called the strength of selection, and it
measures the importance of the relative payoff advantages for the evolutionary
success of a strategy. For small values of \(\beta\), \(\beta \approx 0\),
payoffs become irrelevant, and a strategy's fixation probability approaches
\(\phi_{M} \approx 1 / N\). The larger the value of \(\beta\), the more strongly
the evolutionary process favours the fixation of strategies that yield high
payoffs.
Depending on the fixation probability \(\phi_{M}\) the mutant either fixes
(becomes the new resident) or goes extinct. Regardless, in the elementary time
step another mutant strategy is introduced to the  population. We iterate this
elementary population updating process for a large number of mutant strategies
and we record the resident strategies at each time step.

\christian{For the code, could we provide a link to some online repository? Also, it would be nice to have more information on how we classified strategies as partners in the simulations.} 

{\setlength{\bibsep}{0\baselineskip}
\bibliographystyle{naturemag}
\bibliography{bibliography.bib}
}


\clearpage
\newpage

\begin{figure}[t]
  \centering
  \includegraphics[width=0.75\textwidth]{figures/conceptual_figure_model.pdf}
  \caption{\textbf{The repeated prisoner's dilemma among players with finite memory.}
  \textbf{A,} In the repeated prisoner's dilemma, in each round two players independently decide whether to cooperate~($C$) or to defect~($D$). 
   \textbf{B,} When players adopt memory-1 strategies, their decisions depend on the outcome of the previous round. That is, they consider both their own and the co-player's previous action. 
   \textbf{C,} When players adopt a reactive-$n$ strategy, they make their decisions based on the co-player's actions during the past $n$ rounds. 
   \textbf{D,} A self-reactive-$n$ strategy is contingent on the player's own actions during the past $n$ rounds. 
   \textbf{E,} To illustrate these concepts, we show a game between an arbitrary player (top) and a player with a reactive-1 strategy (bottom). 
   Reactive-1 strategies can be represented as a vector  $\mathbf{p} \!=\! (p_C, p_D)$. 
   The entry $p_C$ is the probability of cooperating after the co-player cooperated in the previous round.
   The entry $p_D$ is the cooperation probability after the co-player defected. 
   \textbf{E,} Now, the bottom player adopts a self-reactive-1 strategy, $\mathbf{\tilde p}\!=\!(\tilde p_C, \tilde p_D)$. 
   Here, the bottom player's cooperation probabilities depend on their own previous action. 
   \christian{Could we change panels E and F such that the first player is the reactive/self-reactive player? It's usually easier to write things if the player being described is player~1. Also, below the other player, could we write 'co-player'?}
   }\label{fig:conceptual_figure_model}
\end{figure}


\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/conceptual_figure_results.pdf}
  \caption{
  \textbf{Characterizing the partners among the reactive-$n$ strategies.} 
{\bf A,B,} To characterize the reactive-$n$ partner strategies, we prove the following result. 
Suppose the focal player adopts a reactive-$n$ strategy. 
Then, for any strategy of the opponent (with arbitrary memory), one can find an associated self-reactive-$n$ strategy that yields the same payoffs. 
Here, we show an example where player~1 uses a reactive-1 strategy against player~2 with a memory-1 strategy. 
Our result implies that can switch to a well-defined self-reactive-1 strategy. 
This switch leaves the outcome distribution unchanged.
In both cases, players are equally likely to experience mutual cooperation, unilateral cooperation, or mutual defection in the long run. 
\textbf{C,} Based on this insight, we can explicitly characterize the reactive-2 partner strategies (with $p_{CC}\!=\!1$). 
Here, we represent the corresponding conditions~\eqref{eq:two_bit_conditions} for a donation game with $b/c\!=\!2$. 
Among the reactive-2 strategies, the counting strategies correspond to the subset with $p_{CD}\!=\!p_{DC}$. 
Counting strategies only depend on how often the co-player cooperated in the past, not on the timing of cooperation.
\textbf{D,} Similarly, we can also characterize the reactive-2 partner strategies for the general prisoner's dilemma. 
Here, we use the values of Axelrod~\citep{axelrod:AAAS:1981}.
\christian{Could we change panels A,B, such that the reactive-1 player is player~2? -- Note that we also have to change the `realized game' and the `outcome distribution' accordingly. Also here, the reason is that things are easier to describe if the focal player is player~1. Here, the focal player is the memory-1 player who switches to a self-reactive-1 strategy.}
}\label{fig:conceptual_figure_results}
\end{figure}


\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/abundant_strategies.pdf}
  \caption{\textbf{Evolutionary dynamics of reactive-$n$ strategies.}
  To explore the evolutionary dynamics among reactive-$n$ strategies, we run simulations based on the
  method of Imhof and Nowak~\cite{imhof:royal:2010}. 
  This method assumes rare mutations. 
  Every time a mutant strategy appears, it goes extinct or fixes before the arrival of the next mutant strategy. 
  {\bf A,B,} We consider ten independent simulations for reactive-$n$ strategies and for reactive-$n$ counting strategies. 
  For each simulation, we record the most abundant strategy (the strategy that resisted most mutants). 
  The respective average cooperation probabilities are in line with the conditions for partner strategies. 
  {\bf C,D,} With additional simulations, we explore the average abundance of partner strategies and the population's average cooperation rate for a range of different cooperation costs and selection strengths. 
  In all cases, we only observe high cooperation rates when partner strategies evolve. 
 Simulations are based on a donation game with \(b\!=\!1\),  \(c\!=\!.5\), and a selection strength parameter $\beta\!=\!1$, unless noted otherwise.     For $n$ equal to 1
  and 2, simulations are run for \(T\!=\! 10 ^ 7\) time steps. For $n\!=\!3$ we use \(T\!=\! 2 \!\cdot\!10 ^ 7\) time steps.
  \christian{Could we correct the legend in panels C and D? Also, I think the figure looks nicer if the legend does not have a grey box around it.}}\label{fig:evolutionary_results}
\end{figure}


\end{document}




